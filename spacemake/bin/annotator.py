import os
import logging
from time import time, sleep
from collections import defaultdict
import pysam
import multiprocessing as mp
import sys
from spacemake.annotation import GenomeAnnotation, CompiledClassifier

from spacemake.parallel import (
    join_with_empty_queues,
    order_results,
    ExceptionLogging,
    queue_iter,
    log_qerr,
    put_or_abort,
    chunkify,
)
import spacemake.util as util

###########################################################
## Here comes the parallel BAM processing implementation ##
###########################################################

def read_BAM_to_queue(
    bam_in, Qsam, Qerr, abort_flag, shared, chunk_size=20000, reader_threads=4
):
    """
    reads from BAM file, converts to string, groups the records into chunks for
    faster parallel processing, and puts these on a mp.Queue()
    """

    def read_source(bam):
        for read in bam.fetch(until_eof=True):
            yield (read.tostring(), read.get_blocks())

    with ExceptionLogging(
        "spacemake.annotator.read_BAM_to_queue", Qerr=Qerr, exc_flag=abort_flag
    ) as el:
        bam_in = util.quiet_bam_open(
            bam_in, "rb", check_sq=False, threads=reader_threads
        )
        shared["bam_header"] = util.make_header(
            bam_in, progname=os.path.basename(__file__)
        )
        # print(f"read process shared={shared}")
        for chunk in chunkify(read_source(bam_in), n_chunk=chunk_size):
            el.logger.debug(
                f"placing {chunk[0]} {len(chunk[1])} in queue of depth {Qsam.qsize()}"
            )
            if put_or_abort(Qsam, chunk, abort_flag):
                el.logger.warning("shutdown flag was raised!")
                break


class AnnotationStats:
    logger = logging.getLogger("spacemake.annotator.stats")
    def __init__(self, mapq=defaultdict(int), flag=defaultdict(int), gn= defaultdict(int), gf= defaultdict(int), gt= defaultdict(int), **kw):
        from collections import defaultdict
        self.mapq = mapq
        self.flag = flag
        self.gn = gn
        self.gf = gf
        self.gt = gt

        self.last_qname = None

    def count(self, read, gn_val, gf_val, gt_val):
        # count each read only once even if we have multiple alignments or mate pairs
        if read.qname != self.last_qname:
            self.flag[read.flag] += 1
            self.mapq[read.mapping_quality] += 1
            self.gf[gf_val] += 1
            self.gt[gt_val] += 1
            self.gn[gn_val] += 1

        self.last_qname = read.qname

    def as_dict(self):
        return dict(
            mapq=self.mapq,
            flag=self.flag,
            gn=self.gn,
            gf=self.gf,
            gt=self.gt,
        )

    def save_stats(self, fname, fields=["mapq", "flag", "gt", "gf", "gn"]):
        import pandas as pd
        fname = util.ensure_path(fname)

        data = []
        for name in fields:
            d = getattr(self, name)
            for obs, count in sorted(d.items(), key=lambda x: -x[1]):
                data.append( (name, obs, count))
        
        self.logger.debug(f"writing annotation statistics to '{fname}'")
        df = pd.DataFrame(data, columns=["field", "value", "count"])
        df.to_csv(fname, sep='\t', index=None)
        return df


def write_BAM_from_queue(
    bam_out, bam_mode, Qres, Qerr, abort_flag, shared, writer_threads=8
):
    """_summary_
    The output generating sub-process. Iterate over the input BAM and gather the annotation tags generated by
    parallel worker sub-processes simultaneously. Then assign the tags to the BAM records and write them to the
    output BAM.

    Args:
        bam_out (str): fila name of output BAM file
        bam_mode (str): mode for output BAM. Can be used to set compression level (e.g. bu or b6)
        Qres (mp.Queue): source for chunks of annotation data
        Qerr (mp.Queue): in case of error, dump exception information onto this queue so that it can be
            extracted from the main process
        abort_flag (mp.Value): inter-process flag to indicate abort conditions (i.e sth went wrong elsewhere)
    """

    with ExceptionLogging(
        "spacemake.annotator.order_results", Qerr=Qerr, exc_flag=abort_flag
    ) as el:

        while not "bam_header" in shared:
            # print(f"writer shared={shared}")
            sleep(0.5)
            if abort_flag.value:
                raise ValueError(
                    "abort_flag was raised before bam_header was received!"
                )

        stats = AnnotationStats()
        header = pysam.AlignmentHeader.from_dict(shared["bam_header"])
        logger = el.logger
        logger.debug(f"writing to new BAM '{bam_out}'")
        bam = pysam.AlignmentFile(
            bam_out, f"w{bam_mode}", header=header, threads=writer_threads
        )
        for aln_str, (gn_val, gf_val, gt_val) in order_results(
            Qres, abort_flag, logger=el.logger
        ):
            # set BAM tags and write
            # (n_read, qname, gn_val, gf_val, gt_val) = annotation
            # assert qname == read.query_name
            # TODO: add the tag values as a string operation, then call fromstring() 
            # set_tag() is costly and causes congestion in Qres which baloons memory usage of the workers
            tagged_str = f"{aln_str}\tgn:Z:{gn_val}\tgf:Z:{gf_val}\tgt:Z:{gt_val}"
            read = pysam.AlignedSegment.fromstring(tagged_str, header)
            # read.set_tag("gn", gn_val)
            # read.set_tag("gf", gf_val)
            # read.set_tag("gt", gt_val)
            # read.set_tag("XF", None)
            # read.set_tag("gs", None)
            bam.write(read)

            # keep statistics
            stats.count(read, gn_val, gf_val, gt_val)
    # share the statistics with the parent process
    shared.update(stats.as_dict())


def annotate_chunks_from_queue(compiled_annotation, Qsam, Qres, Qerr, abort_flag):
    """_summary_
    Worker sub-process. Iterate over the BAM file independently, skip everything that is not in our chunk
    (every rr_i'th chunk) if chunk_size BAM records. Annotate our chunk using compiled GenomeAnnotation
    and place complete gn, gf, gt string values on a results Queue.

    Args:
        compiled_annotation (str): path to directory with compiled annotation information
        Qres (mp.Quere): Queue to place chunks of complete annotation tags in
        Qerr (mp.Queue): see merge_annotations_from_queue_with_BAM
        rr_n (int): how many round-robin workers are there in total
        rr_i (int): which round-robin worker are we? 0..rr_n-1 . Determines which
            chunks this worker feels responsible for.
        abort_flag (mp.Value): see merge_annotations_from_queue_with_BAM
        chunk_size (int): number of consecutive BAM records to process
    """
    with ExceptionLogging(
        "spacemake.annotator.annotate_chunks_from_queue", Qerr=Qerr, exc_flag=abort_flag
    ) as el:
        logger = logging.getLogger("spacemake.annotator.annotate_chunks_from_queue")
        ga = GenomeAnnotation.from_compiled_index(compiled_annotation)

        for n_chunk, data in queue_iter(Qsam, abort_flag):
            # annotate a chunk of BAM records, which have been converted to strings
            out = []
            for aln_str, blocks in data:
                sam = aln_str.split("\t")
                flags = int(sam[1])
                is_unmapped = flags & 4
                gn = None
                gf = "-"
                gt = None
                if not is_unmapped:
                    chrom = sam[2]
                    strand = "-" if (flags & 16) else "+"
                    gn, gf, gt = ga.get_annotation_tags(chrom, strand, blocks)

                out.append((aln_str, (gn, gf, gt)))

            Qres.put((n_chunk, out))

##############################################################
### Here are the main functions as offered via commandline ###
##############################################################

def annotate_BAM_parallel(args):
    """_summary_
    Main function of the 'annotate' command. Create the plumbing for parallel worker processes and a single
    collector/writer process.

    Args:
        args (namespace): the command-line arguments reported from the parser
    """

    Qsam = mp.Queue(10 * args.parallel)  # BAM records, converted to strings, are place here in chunks
    Qres = mp.Queue(10 * args.parallel)  # chunks are passed onto this queue, together with tags
    Qerr = mp.Queue()  # child-processes can report errors back to the main process here

    manager = mp.Manager()
    shared = manager.dict()

    # Proxy objects to allow workers to report statistics about the run
    abort_flag = mp.Value("b")
    abort_flag.value = False

    with ExceptionLogging(
        "spacemake.annotator.annotate_BAM_parallel", exc_flag=abort_flag
    ) as el:

        if not args.bam_in:
            raise ValueError("you need to provide a BAM file as input!")

        # bam_in, Qsam, Qerr, abort_flag, shared, chunk_size=20000, reader_threads=4
        reader = mp.Process(
            target=read_BAM_to_queue,
            name="reader",
            args=(args.bam_in, Qsam, Qerr, abort_flag, shared, args.chunk_size),
        )
        reader.start()
        el.logger.debug("started BAM reader")
        # sleep(2)
        # print("main process shared=", shared)
        workers = []
        for i in range(args.parallel):
            # compiled_annotation, Qsam, Qres, Qerr, abort_flag
            w = mp.Process(
                target=annotate_chunks_from_queue,
                name=f"annotator_{i}",
                args=(args.compiled, Qsam, Qres, Qerr, abort_flag),
            )
            w.start()
            workers.append(w)

        el.logger.debug("Started workers")
        # bam_out, bam_mode, Qres, Qerr, abort_flag, shared, writer_threads=8
        collector = mp.Process(
            target=write_BAM_from_queue,
            name="output",
            args=(args.bam_out, args.bam_mode, Qres, Qerr, abort_flag, shared),
        )
        collector.start()
        el.logger.debug("Started collector")

        # wait until all sequences have been thrown onto Qfq
        qsam, qerr = join_with_empty_queues(reader, [Qsam, Qerr], abort_flag)
        el.logger.debug("The reader exited")
        if qsam or qerr:
            el.logger.error(f"{len(qsam)} chunks were drained from Qsam upon abort.")
            log_qerr(qerr)

        # signal all workers to finish
        el.logger.debug("Signalling all workers to finish")
        for _ in range(args.parallel):
            Qsam.put(None)  # each worker consumes exactly one None

        for w in workers:
            # make sure all results are on Qres by waiting for
            # workers to exit. Or, empty queues if aborting.
            qres, qerr = join_with_empty_queues(w, [Qres, Qerr], abort_flag)
            if qres or qerr:
                el.logger.error(
                    f"{len(qres)} chunks were drained from Qres upon abort."
                )
                log_qerr(qerr)

        el.logger.debug(
            "All worker processes have joined. Signalling collector to finish."
        )
        # signal the collector to stop
        Qres.put(None)

        # and wait until all output has been generated
        collector.join()
        el.logger.debug("Collector has joined. Merging worker statistics.")
        if args.stats_out:
            stats = AnnotationStats(**shared)
            stats.save_stats(args.stats_out)

    if el.exception:
        ret_code = -1
    else:
        ret_code = 0

    return ret_code


def build_compiled_annotation(args):
    logger = logging.getLogger("spacemake.annotator.build_compiled_annotation")
    if CompiledClassifier.files_exist(args.compiled):
        logger.warning(
            "already found a compiled annotation. use --force-overwrite to overwrite"
        )
        # ga = GenomeAnnotation.from_compiled_index(args.compiled)
        if not args.force_overwrite:
            return

    if args.tabular and os.access(args.tabular, os.R_OK):
        ga = GenomeAnnotation.from_uncompiled_df(args.tabular)
    else:
        ga = GenomeAnnotation.from_GTF(args.gtf, df_cache=args.tabular)

    ga = ga.compile(args.compiled)


def query_regions(args):
    logger = logging.getLogger("spacemake.annotator.query_regions")
    if args.compiled:
        ga = GenomeAnnotation.from_compiled_index(args.compiled)
    else:
        ga = GenomeAnnotation.from_GTF(args.gtf)

    for region in args.region:
        logger.debug(f"querying region '{region}'")
        chrom, coords, strand = region.split(":")
        start, end = coords.split("-")

        gn, gf, gt = ga.get_annotation_tags(
            chrom,
            strand,
            [
                (int(start), int(end)),
            ]
        )
        print(f"gn={gn}\tgf={gf}\tgt={gt}")


def query_gff(args):
    logger = logging.getLogger("spacemake.annotator.query_gff")
    if args.compiled:
        ga = GenomeAnnotation.from_compiled_index(args.compiled)
    else:
        ga = GenomeAnnotation.from_GTF(args.gtf)

    for line in open(args.gff):
        if line.startswith('#'):
            continue
        
        parts = line.split("\t")
        if len(parts) != 9:
            # not a GTF/GFF formatted line
            continue

        chrom, source, feature, start, end, score, strand, frame, attr_str = parts

        start = int(start) - 1
        end = int(end)

        logger.debug(f"querying region '{chrom}:{start}-{end}:{strand}'")

        gn, gf, gt = ga.get_annotation_tags(
            chrom,
            strand,
            [
                (start, end),
            ],
        )
        print(f'{line.rstrip()} gn "{gn}"; gf "{gf}"; gt "{gt}";')



def parse_args():
    parser = util.make_minimal_parser("annotator.py")  # argparse.ArgumentParser()

    def usage(args):
        parser.print_help()

    parser.set_defaults(func=usage)

    subparsers = parser.add_subparsers()

    build_parser = subparsers.add_parser("build")
    build_parser.set_defaults(func=build_compiled_annotation)
    build_parser.add_argument(
        "--gtf",
        default=None,
        required=True,
        help="path to the original annotation (e.g. gencodev38.gtf.gz)",
    )
    build_parser.add_argument(
        "--compiled",
        default=None,
        help="path to a directoy in which a compiled version of the GTF is stored",
    )
    build_parser.add_argument(
        "--tabular",
        default="",
        help="path to a cache of the tabular version of the relevant GTF features (optional)",
    )
    build_parser.add_argument(
        "--force-overwrite",
        default=False,
        action="store_true",
        help="re-compile GTF and overwrite the pre-existing compiled annotation",
    )

    tag_parser = subparsers.add_parser("tag")
    tag_parser.set_defaults(func=annotate_BAM_parallel)
    tag_parser.add_argument(
        "--compiled",
        default=None,
        help="path to a directoy in which a compiled version of the GTF is stored",
    )
    tag_parser.add_argument("--bam-in", default="", help="path to the input BAM")
    tag_parser.add_argument(
        "--bam-out", default="", help="path for the tagged BAM output"
    )
    tag_parser.add_argument(
        "--bam-mode", default="b", help="mode of the output BAM file (default=b)"
    )
    tag_parser.add_argument(
        "--stats-out", default="", help="path for statistics output"
    )
    tag_parser.add_argument(
        "--parallel",
        type=int,
        default=4,
        help="how many parallel annotation processes to create (default=4)",
    )
    tag_parser.add_argument(
        "--chunk-size",
        type=int,
        default=20000,
        help="how many BAM-records form a chunk for parallel processing (default=20000)",
    )
    query_parser = subparsers.add_parser("query")
    query_parser.set_defaults(func=query_regions)
    query_parser.add_argument(
        "--compiled",
        default=None,
        help="path to a directoy in which a compiled version of the GTF is stored",
    )
    query_parser.add_argument(
        "--gtf",
        default=None,
        help="path to the original annotation (e.g. gencodev38.gtf.gz)",
    )
    query_parser.add_argument("region", default=[], help="region to query", nargs="+")

    gff_parser = subparsers.add_parser("gff")
    gff_parser.set_defaults(func=query_gff)
    gff_parser.add_argument(
        "--compiled",
        default=None,
        help="path to a directoy in which a compiled version of the GTF is stored",
    )
    gff_parser.add_argument(
        "--gff",
        default=None,
        help="path to the gff file you want queried",
    )

    return parser.parse_args()


def cmdline():
    args = parse_args()
    util.setup_logging(args, "spacemake.annotator.cmdline")
    return args.func(args)


if __name__ == "__main__":
    ret_code = cmdline()
    sys.exit(ret_code)

    # import cProfile
    # cProfile.run("cmdline()", "prof_stats")
